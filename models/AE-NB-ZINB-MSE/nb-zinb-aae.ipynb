{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os, random\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import anndata\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "import tensorflow as tf\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d400b747fd1a6147",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from tensorflow.python.framework.ops import enable_eager_execution, disable_eager_execution"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f35fbbf239553410",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "disable_eager_execution()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2edf5a63136217b4",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1aba4f2267c6981f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "noise_file = \"sim_g2_dropout_5\"\n",
    "\n",
    "obs_label_column = \"Group\"\n",
    "\n",
    "adata = sc.read_h5ad(f\"C:\\\\Users\\\\gmaho\\\\Desktop\\\\ai_masters_thesis\\\\simulated_datasets\\\\{noise_file}.h5ad\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2fb847619c9c6e04",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sim_raw = adata\n",
    "\n",
    "y = np.array(adata.obs[obs_label_column])\n",
    "\n",
    "sim_true = sc.AnnData(adata.layers[\"TrueCounts\"],obs=pd.DataFrame(y, columns=[obs_label_column]))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae9154076d9d0fce",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sc.pp.filter_genes(sim_raw, min_counts=1)\n",
    "\n",
    "sim_true\n",
    "\n",
    "sim_raw_norm = sim_raw.copy()\n",
    "sc.pp.normalize_total(sim_raw_norm)\n",
    "sc.pp.log1p(sim_raw_norm)\n",
    "sc.pp.pca(sim_raw_norm)\n",
    "\n",
    "sim_true_norm = sim_true.copy()\n",
    "sc.pp.normalize_total(sim_true_norm)\n",
    "sc.pp.log1p(sim_true_norm)\n",
    "sc.pp.pca(sim_true_norm)\n",
    "\n",
    "# print(dropout_gt[:10, :10])\n",
    "print(sim_raw)\n",
    "print(sim_true)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a78678c9c3a39d15",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Denoise Simulated Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "731a434fdb89b3f3"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8c208f5d417027ff"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def run_prep(adata,\n",
    "        mode='denoise',\n",
    "        hidden_size=(64, 32, 64), \n",
    "        hidden_dropout=0.,\n",
    "        batchnorm=True,\n",
    "        activation='relu',\n",
    "        init='glorot_uniform',\n",
    "        epochs=300, \n",
    "        reduce_lr=10,\n",
    "        early_stop=20,\n",
    "        batch_size=32,\n",
    "        optimizer='RMSprop',\n",
    "        learning_rate=None,\n",
    "        random_state=0,\n",
    "        threads=None,\n",
    "        verbose=False,\n",
    "        ):\n",
    "    \n",
    "\n",
    "    assert isinstance(adata, anndata.AnnData), 'adata must be an AnnData instance'\n",
    "    assert mode in ('denoise', 'latent'), '%s is not a valid mode.' % mode\n",
    "\n",
    "    # set seed for reproducibility\n",
    "    random.seed(random_state)\n",
    "    np.random.seed(random_state)\n",
    "    tf.random.set_seed(random_state)\n",
    "\n",
    "    # check for zero genes\n",
    "    nonzero_genes, _ = sc.pp.filter_genes(adata.X, min_counts=1)\n",
    "    assert nonzero_genes.all(), 'Please remove all-zero genes before using DCA.'\n",
    "\n",
    "    adata.raw = adata.copy()\n",
    "\n",
    "    sc.pp.normalize_total(adata, key_added=\"n_counts\")\n",
    "    adata.obs[\"sf\"] = adata.obs.n_counts / np.median(adata.obs.n_counts)\n",
    "\n",
    "    sc.pp.scale(adata)\n",
    "\n",
    "    input_size = output_size = adata.n_vars\n",
    "\n",
    "    network_keywords = {\n",
    "                    'input_size': input_size,\n",
    "                    'output_size': output_size,\n",
    "                    'hidden_size': hidden_size,\n",
    "                    'hidden_dropout': hidden_dropout,\n",
    "                    'batchnorm': batchnorm,\n",
    "                    'activation': activation,\n",
    "                    'init': init\n",
    "                    }\n",
    "\n",
    "    \n",
    "\n",
    "    # ------------------------------------------------\n",
    "    training_keywords = {\n",
    "                     'epochs': epochs,\n",
    "                     'reduce_lr': reduce_lr,\n",
    "                     'early_stop': early_stop,\n",
    "                     'batch_size': batch_size,\n",
    "                     'optimizer': optimizer,\n",
    "                     'verbose': verbose,\n",
    "                     'threads': threads,\n",
    "                     'learning_rate': learning_rate\n",
    "                     }\n",
    "    #-----------------------------------------------------------------------\n",
    "    # adata.obs['split'] = 'train'\n",
    "\n",
    "    # training_data = adata[adata.obs.split == 'train']\n",
    "\n",
    "    return adata, network_keywords, training_keywords"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "192d2d2ae3948936",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "adata, network_keywords, training_keywords = run_prep(sim_raw)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "17577d7601431307",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "network_keywords"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "98abc165ccc4ddde",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Building a basic AE model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f8bd42774bfc138"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d54254fda0d376d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scanpy as sc\n",
    "\n",
    "import keras\n",
    "from keras.layers import Input, Dense, Activation, BatchNormalization, Lambda, Layer\n",
    "from keras.models import Model\n",
    "from keras.losses import MeanSquaredError\n",
    "\n",
    "import tensorflow as tf"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d1d4bb460479917",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def _nan_to_infinite(x):\n",
    "    return tf.where(tf.math.is_nan(x), tf.zeros_like(x)+np.inf, x)\n",
    "\n",
    "Multiply = Lambda(lambda l: l[0]*tf.reshape(l[1], (-1,1)))\n",
    "MeanActivation = lambda x: tf.clip_by_value(tf.math.exp(x), 1e-5, 1e6)\n",
    "VarActivation = lambda x: tf.clip_by_value(tf.nn.softplus(x), 1e-4, 1e4)\n",
    "\n",
    "class SliceLayer(Layer):\n",
    "    def __init__(self, index, **kwargs):\n",
    "        self.index = index\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if not isinstance(input_shape, list):\n",
    "            raise ValueError('Input should be a list')\n",
    "\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        assert isinstance(x, list), 'SliceLayer input is not a list'\n",
    "        return x[self.index]\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[self.index]\n",
    "\n",
    "# Negative Binomial Loss Function attempt\n",
    "class NbLoss(object):\n",
    "    def __init__(self, theta=None, scope=\"nb-loss/\",\n",
    "                 scale=1.0):\n",
    "        \n",
    "        # epsilon present for stability\n",
    "        self.epsilon = 1e-10\n",
    "        self.scale = scale\n",
    "        self.theta = theta\n",
    "        self.scope = scope\n",
    "\n",
    "    def loss(self, y_true, y_pred, mean=True):\n",
    "        scale = self.scale\n",
    "        epsilon = self.epsilon\n",
    "\n",
    "        with tf.name_scope(self.scope):\n",
    "            y_true = tf.cast(y_true, tf.float32)\n",
    "            y_pred = tf.cast(y_pred, tf.float32) * scale\n",
    "\n",
    "            theta = tf.minimum(self.theta, 1e6)\n",
    "\n",
    "            term1 = tf.math.lgamma(theta+epsilon) + tf.math.lgamma(y_true+1.0) - tf.math.lgamma(y_true+theta+epsilon)\n",
    "            term2 = (theta+y_true) * tf.math.log(1.0 + (y_pred/(theta+epsilon))) + (y_true * (tf.math.log(theta+epsilon) - tf.math.log(y_pred+epsilon)))\n",
    "\n",
    "            final = term1 + term2\n",
    "\n",
    "            final = _nan_to_infinite(final)\n",
    "\n",
    "            if mean:\n",
    "                final = tf.reduce_mean(final)\n",
    "\n",
    "\n",
    "        return final\n",
    "\n",
    "# Zero inflated Negative Binomial Loss Function\n",
    "class ZiNbLoss(NbLoss):\n",
    "    def __init__(self, pmw, lambda_=0.0, scope=\"zinb-loss\", **kwargs):\n",
    "        super().__init__(scope=scope, **kwargs)\n",
    "        self.pmw = pmw\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "    def loss(self, y_true, y_pred, mean=True):\n",
    "        scale = self.scale\n",
    "        epsilon = self.epsilon\n",
    "\n",
    "        with tf.name_scope(self.scope):\n",
    "            nb_case = super().loss(y_true, y_pred, mean=False) - tf.math.log(1.0-self.pmw+epsilon)\n",
    "\n",
    "            y_true = tf.cast(y_true, tf.float32)\n",
    "            y_pred = tf.cast(y_pred, tf.float32) * scale\n",
    "            theta = tf.minimum(self.theta, 1e6)\n",
    "\n",
    "            zero_nb = tf.pow(theta/(theta+y_pred+epsilon), theta)\n",
    "            zero_case = -tf.math.log(self.pmw + ((1.0-self.pmw)*zero_nb)+epsilon)\n",
    "            result = tf.where(tf.less(y_true, 1e-8), zero_case, nb_case)\n",
    "            r = self.lambda_*tf.square(self.pmw)\n",
    "            result += r\n",
    "\n",
    " \n",
    "            result = tf.reduce_mean(result)\n",
    "\n",
    "            result = _nan_to_infinite(result)\n",
    "\n",
    "        return result"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "772c5e3351dbb8aa",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Create the basic Autoencoder\n",
    "def create_autoencoder(loss_type=\"mse\"):\n",
    "    # Step 1, defining the autoencoder\n",
    "    input = Input(shape=(network_keywords[\"input_size\"],), name='counts')\n",
    "    sf = Input(shape=(1,), name='sf')\n",
    "\n",
    "    x = Dense(64, activation=None, kernel_initializer=network_keywords[\"init\"], name=\"encoder0\")(input)\n",
    "\n",
    "    x = BatchNormalization(center=True, scale=False)(x)\n",
    "\n",
    "    x = Activation(network_keywords[\"activation\"], name=\"relu_encoder0\")(x)\n",
    "\n",
    "    x = Dense(32, activation=None, kernel_initializer=network_keywords[\"init\"], name=\"latent0\")(x)\n",
    "\n",
    "    x = BatchNormalization(center=True, scale=False)(x)\n",
    "\n",
    "    x = Activation(network_keywords[\"activation\"], name=\"relu_latent0\")(x)\n",
    "\n",
    "    x = Dense(64, activation=None, kernel_initializer=network_keywords[\"init\"], name=\"decoder0\")(x)\n",
    "\n",
    "    x = BatchNormalization(center=True, scale=False)(x)\n",
    "\n",
    "    x = Activation(network_keywords[\"activation\"], name=\"relu_decoder0\")(x)\n",
    "\n",
    "    if loss_type == \"mse\":\n",
    "        mean = Dense(network_keywords[\"input_size\"], kernel_initializer=network_keywords[\"init\"],\n",
    "                        name='mean')(x)\n",
    "\n",
    "        output = Multiply([mean, sf])\n",
    "\n",
    "        return keras.Model(inputs=[input,sf], outputs=output, name=\"mse_net\"), None\n",
    "\n",
    "    elif loss_type == \"nb\":\n",
    "        var = Dense(network_keywords[\"input_size\"], activation=VarActivation,\n",
    "                           kernel_initializer=network_keywords[\"init\"],\n",
    "                           name='variance')(x)\n",
    "\n",
    "        mean = Dense(network_keywords[\"input_size\"], activation=MeanActivation, \n",
    "                     kernel_initializer=network_keywords[\"init\"],\n",
    "                       name='mean')(x)\n",
    "        \n",
    "        output = Multiply([mean, sf])\n",
    "        output = SliceLayer(0, name='slice')([output, var])\n",
    "\n",
    "        nb = NbLoss(theta=var)\n",
    "        loss = nb.loss\n",
    "\n",
    "        return keras.Model(inputs=[input, sf], outputs=output, name=\"nb_net\"), loss\n",
    "    \n",
    "    elif loss_type == \"zinb\":\n",
    "        # pmw = point mass weight\n",
    "        pmw = Dense(network_keywords[\"input_size\"], activation='sigmoid', kernel_initializer=network_keywords[\"init\"],\n",
    "                       name='pi')(x)\n",
    "        \n",
    "        var = Dense(network_keywords[\"input_size\"], activation=VarActivation,\n",
    "                           kernel_initializer=network_keywords[\"init\"],\n",
    "                           name='variance')(x)\n",
    "\n",
    "        mean = Dense(network_keywords[\"input_size\"], activation=MeanActivation, kernel_initializer=network_keywords[\"init\"],\n",
    "                       name='mean')(x)\n",
    "        \n",
    "        output = Multiply([mean, sf])\n",
    "        output = SliceLayer(0, name='slice')([output, var, pmw])\n",
    "\n",
    "        zinb = ZiNbLoss(pmw, theta=var, lambda_=0)\n",
    "        loss = zinb.loss\n",
    "\n",
    "        return keras.Model(inputs=[input, sf], outputs=output, name=\"zinb_net\"), loss"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c185af7c6e6ab16",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def run(loss_type=\"zinb\"): # loss_type can be mse, nb, zinb\n",
    "    autoencoder, loss_fn = create_autoencoder(loss_type=loss_type)\n",
    "    \n",
    "    if not loss_fn and loss_type == \"mse\":\n",
    "        loss_fn = MeanSquaredError()\n",
    "\n",
    "    autoencoder.compile(\n",
    "        optimizer=keras.optimizers.RMSprop(lr=1e-3, clipvalue=5),\n",
    "        loss=loss_fn,) #loss_fn\n",
    "    \n",
    "    input_data = {'counts': adata.X, 'sf': adata.obs.sf}\n",
    "    output_data = adata.raw.X\n",
    "    \n",
    "    verbose=False\n",
    "    lr_patience=10\n",
    "    es_patience=15\n",
    "    \n",
    "    callbacks = [ReduceLROnPlateau(monitor='val_loss', patience=lr_patience, verbose=verbose), EarlyStopping(monitor='val_loss', patience=es_patience, verbose=verbose)]\n",
    "    autoencoder.fit(input_data, output_data, batch_size=32, epochs=300, validation_split=0.1, callbacks=callbacks)\n",
    "    \n",
    "    result = autoencoder.predict({'counts': adata.X, 'sf': adata.obs.sf})\n",
    "    \n",
    "    denoised_adata = sc.AnnData(result,obs=pd.DataFrame(y, columns=[obs_label_column]))\n",
    "    \n",
    "    denoised_adata_norm = denoised_adata.copy()\n",
    "    \n",
    "    sc.pp.normalize_total(denoised_adata_norm)\n",
    "    sc.pp.log1p(denoised_adata_norm)\n",
    "    \n",
    "    if loss_type == \"mse\":\n",
    "        denoised_adata_norm.X = np.nan_to_num(denoised_adata_norm.X, nan=0.1, posinf=0, neginf=0)\n",
    "        \n",
    "    return autoencoder, denoised_adata_norm"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b248afe011257dd",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "autoencoder, denoised_adata_norm = run(loss_type=\"zinb\")\n",
    "\n",
    "sc.pp.pca(denoised_adata_norm)\n",
    "    \n",
    "sc.pl.pca_scatter(denoised_adata_norm, color='Group', size=20, title=\"Denoised(ZINB)\", show=False, legend_loc='none')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ffa7c7590edc0c4",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ebfa674b2cbfd620",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "# Generate silhouette scores barplot\n",
    "d = {\"No Dropout\": [], \"Dropout\": [],\"ZINB\": [], \"NB\": [], \"MSE\": [], }\n",
    "# sim_raw_norm = sim_raw.copy()\n",
    "\n",
    "d[\"No Dropout\"].append(silhouette_score(sim_true_norm.obsm['X_pca'][:, :2], \n",
    "                 sim_true_norm.obs.Group))\n",
    "\n",
    "d[\"Dropout\"].append(silhouette_score(sim_raw_norm.obsm['X_pca'][:, :2], \n",
    "                 sim_raw_norm.obs.Group))\n",
    "\n",
    "for i in range(0, 1):    \n",
    "    ae, zinb_r = run(loss_type=\"zinb\")\n",
    "    \n",
    "    sc.pp.pca(zinb_r)\n",
    "    \n",
    "    d[\"ZINB\"].append(silhouette_score(zinb_r.obsm['X_pca'][:, :2], \n",
    "                 zinb_r.obs.Group))\n",
    "    \n",
    "    ae, nb_r = run(loss_type=\"nb\")\n",
    "        \n",
    "    sc.pp.pca(nb_r)\n",
    "    \n",
    "    d[\"NB\"].append(silhouette_score(nb_r.obsm['X_pca'][:, :2], \n",
    "                 nb_r.obs.Group))\n",
    "    \n",
    "    ae, mse_r = run(loss_type=\"mse\")\n",
    "    \n",
    "    sc.pp.pca(mse_r)\n",
    "    \n",
    "    d[\"MSE\"].append(silhouette_score(mse_r.obsm['X_pca'][:, :2], \n",
    "                 nb_r.obs.Group))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "325e90bef43a532a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df = pd.DataFrame([(k, v) for k, values in d.items() for v in values], columns=['Category', 'Silhouette Score'])\n",
    "\n",
    "# Display the DataFrame\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3144b40f945536dd",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df\n",
    "plt.xticks(rotation=30)\n",
    "sns.barplot(data=df, x=\"Category\", y=\"Silhouette Score\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad89784b848fb5d6",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "cee04150be6f1af8",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
